---
kernelspec:
  display_name: Julia 1
  language: julia
  name: julia-1.11
numbering:
  headings: false
---
# Chapter 8

## Functions

(function-poweriter)=


## Examples

```{code-cell}
:tags: [remove-output]
import Pkg; Pkg.activate("/Users/driscoll/Documents/GitHub/fnc")
using FundamentalsNumericalComputation
FNC.init_format()
```

### 8.1 @section-krylov-structure
(demo-structure-sparse)=
#### 
Here we load the adjacency matrix of a graph with 2790 nodes. Each node is a web page referring to Roswell, NM, and the edges represent links between web pages. (Credit goes to Panayiotis Tsaparas and the University of Toronto for making this data public.)

```{code-cell}
@load "roswell.jld2" A;      # file is on the book's website
```

```{index} ! Julia; nnz
```
::::{grid} 1 1 2 2

:::{grid-item}


We may define the density of $\mathbf{A}$ as the number of nonzeros divided by the total number of entries.


:::
:::{card}


Use `nnz` to count the number of nonzeros in a sparse matrix.

:::
::::

```{code-cell}
m, n = size(A)
@show density = nnz(A) / (m * n);
```

```{index} ! Julia; summarysize
```

The computer memory consumed by any variable can be discovered using `summarysize`. We can use it to compare the space needed for the sparse representation to its dense counterpart, that is, the space needed to store all the elements, whether zero or not.

```{code-cell}
F = Matrix(A)
Base.summarysize(F) / Base.summarysize(A)
```

As you can see, the storage savings are dramatic. Matrix-vector products are also much faster using the sparse form because operations with structural zeros are skipped.

```{code-cell}
x = randn(n)
A * x;   # make sure * is loaded and compiled
@elapsed for i in 1:300
    A * x
end
```

```{code-cell}
F * x;
@elapsed for i in 1:300
    F * x
end
```

(demo-structure-fill)=
#### 
Here is the adjacency matrix of a graph representing a small-world network, featuring connections to neighbors and a small number of distant contacts.

```{code-cell}
@load "smallworld.jld2" A
graphplot(A, linealpha=0.5)
```

Because each node connects to relatively few others, the adjacency matrix is quite sparse.

```{code-cell}
spy(A, title="Nonzero locations", m=2, color=:blues)
```

By {numref}`Theorem {number} <theorem-insight-adjmat>`, the entries of $\mathbf{A}^k$ give the number of walks of length $k$ between pairs of nodes, as with "*k* degrees of separation" within a social network. As $k$ grows, the density of $\mathbf{A}^k$ also grows.

```{code-cell}
plt = plot(layout=(1, 3), legend=:none, size=(600, 240))
for k in 2:4
    spy!(A^k, subplot=k - 1, color=:blues,
        title=latexstring("\\mathbf{A}^$k"))
end
plt
```

(demo-structure-banded)=
####
```{index} ! Julia; spdiagm
```

The `spdiagm` function creates a sparse matrix given its diagonal elements. The main or central diagonal is numbered zero, above and to the right of that is positive, and below and to the left is negative.

```{code-cell}
n = 50;
A = spdiagm(-3 => fill(n, n - 3),
    0 => ones(n),
    1 => -(1:n-1),
    5 => fill(0.1, n - 5))
Matrix(A[1:7, 1:7])
```

```{index} ! Julia; sparse
```

::::{grid} 1 1 2 2

:::{grid-item}


Without pivoting, the LU factors have the same lower and upper bandwidth as the orignal matrix.


:::
:::{card}


The `sparse` function converts any matrix to sparse form. But it's usually better to construct a sparse matrix directly, as the standard form might not fit in memory.

:::
::::

```{code-cell}
L, U = FNC.lufact(A)
plot(layout=2)
spy!(sparse(L), m=2, subplot=1, title=L"\mathbf{L}", color=:blues)
spy!(sparse(U), m=2, subplot=2, title=L"\mathbf{U}", color=:blues)
```

However, if we introduce row pivoting, bandedness may be expanded or destroyed.

```{code-cell}
fact = lu(A)
plot(layout=2)
spy!(sparse(fact.L), m=2, subplot=1, title=L"\mathbf{L}", color=:blues)
spy!(sparse(fact.U), m=2, subplot=2, title=L"\mathbf{U}", color=:blues)
```

(demo-structure-linalg)=
####
The following generates a random sparse matrix with prescribed eigenvalues.

```{code-cell}
n = 4000
density = 4e-4
λ = @. 1 + 1 / (1:n)   # exact eigenvalues
A = FNC.sprandsym(n, density, λ);
```

```{index} ! Julia; eigs
```

The `eigs` function finds a small number eigenvalues meeting some criterion. First, we ask for the 5 of largest (complex) magnitude using `which=:LM`.

```{code-cell}
λmax, V = eigs(A, nev=5, which=:LM)    # Largest Magnitude
fmt = ft_printf("%20.15f")
pretty_table([λmax λ[1:5]], header=["found", "exact"], formatters=fmt)
```

Now we find the 5 closest to the value 1 in the complex plane, via `sigma=1`.

```{code-cell}
λ1, V = eigs(A, nev=5, sigma=1)    # closest to sigma
data = [λ1 λ[end:-1:end-4]]
pretty_table(data, header=["found", "exact"], formatters=fmt)
```

```{index} Julia; \\
```

The time needed to solve a sparse linear system is not easy to predict unless you have some more information about the matrix. But it will typically be orders of magnitude faster than the dense version of the same problem.

```{code-cell}
x = @. 1 / (1:n);
b = A * x;
```

```{code-cell}
norm(x - A \ b);  # force compilation
t = @elapsed sparse_err = norm(x - A \ b)
println("Time for sparse solve: $t")
```

```{code-cell}
D = Matrix(A)  # convert to regular matrix
norm(x - D \ b);
t = @elapsed dense_err = norm(x - D \ b)
println("Time for dense solve: $t")
```

```{code-cell}
@show sparse_err;
@show dense_err;
```

### 8.2 @section-krylov-power
(demo-power-one)=
####
Here we choose a random 5×5 matrix and a random 5-vector.

```{code-cell}
A = rand(1.0:9.0, 5, 5)
A = A ./ sum(A, dims=1)
x = randn(5)
```

Applying matrix-vector multiplication once doesn't do anything recognizable.

```{code-cell}
y = A * x
```

Repeating the multiplication still doesn't do anything obvious.

```{code-cell}
z = A * y
```

But if we keep repeating the matrix-vector multiplication, something remarkable happens: $\mathbf{A} \mathbf{x} \approx \mathbf{x}$.

```{code-cell}
for j in 1:8
    x = A * x
end
[x A * x]
```

This phenomenon seems to occur regardless of the starting vector.

```{code-cell}
x = randn(5)
for j in 1:8
    x = A * x
end
[x A * x]
```

(demo-power-iter)=
####
We will experiment with the power iteration on a 5×5 matrix with prescribed eigenvalues and dominant eigenvalue at 1.

```{code-cell}
λ = [1, -0.75, 0.6, -0.4, 0]
# Make a triangular matrix with eigenvalues on the diagonal.
A = triu(ones(5, 5), 1) + diagm(λ)
```

We run the power iteration 60 times. The best estimate of the dominant eigenvalue is the last entry of the first output.

```{code-cell}
β, x = FNC.poweriter(A, 60)
eigval = β[end]
```

We check for linear convergence using a log-linear plot of the error.

```{code-cell}
err = @. 1 - β
plot(0:59, abs.(err), m=:o, title="Convergence of power iteration",
    xlabel=L"k", yaxis=(L"|\lambda_1-\beta_k|", :log10, [1e-10, 1]))
```

The asymptotic trend seems to be a straight line, consistent with linear convergence. To estimate the convergence rate, we look at the ratio of two consecutive errors in the linear part of the convergence curve. The ratio of the first two eigenvalues should match the observed rate.

```{code-cell}
@show theory = λ[2] / λ[1];
@show observed = err[40] / err[39];
```

Note that the error is supposed to change sign on each iteration. The effect of these alternating signs is that estimates oscillate around the exact value.

```{code-cell}
β[26:30]
```

In practical situations, we don't know the exact eigenvalue that the algorithm is supposed to find. In that case we would base errors on the final $\beta$ that was found, as in the following plot.

```{code-cell}
err = @. β[end] - β[1:end-1]
plot(0:58, abs.(err), m=:o, title="Convergence of power iteration",
    xlabel=L"k", yaxis=(L"|\beta_{60}-\beta_k|", :log10, [1e-10, 1]))
```

The results are very similar until the last few iterations, when the limited accuracy of the reference value begins to show. That is, while it is a good estimate of $\lambda_1$, it is less good as an estimate of the error in nearby estimates.


### 8.3 @section-krylov-inviter

### 8.4 @section-krylov-subspace

### 8.5 @section-krylov-gmres

### 8.6 @section-krylov-minrescg

### 8.7 @section-krylov-matrixfree

### 8.8 @section-krylov-precond


