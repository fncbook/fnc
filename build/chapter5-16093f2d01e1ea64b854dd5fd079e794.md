---
kernelspec:
  display_name: Julia 1
  language: julia
  name: julia-1.11
---
```{code-cell}
:tags: [remove-cell]
import Pkg; Pkg.activate("/Users/driscoll/Documents/GitHub/fnc")
using FundamentalsNumericalComputation
FNC.init_format()
```

(demo-interpolation-global-julia)=
``````{dropdown} Trouble in polynomial interpolation
Here are some points that we could consider to be observations of an unknown function on $[-1,1]$.

```{code-cell}
n = 5
t = range(-1, 1, length=n + 1)
y = @. t^2 + t + 0.05 * sin(20 * t)

scatter(t, y, label="data", leg=:top)
```

```{index} ! Julia; fit
```

The polynomial interpolant, as computed using `fit`, looks very sensible. It's the kind of function you'd take home to meet your parents.

```{code-cell}
p = Polynomials.fit(t, y, n)     # interpolating polynomial
plot!(p, -1, 1, label="interpolant")
```

But now consider a different set of points generated in almost exactly the same way.

```{code-cell}
n = 18
t = range(-1, 1, length=n + 1)
y = @. t^2 + t + 0.05 * sin(20 * t)

scatter(t, y, label="data", leg=:top)
```

The points themselves are unremarkable. But take a look at what happens to the polynomial interpolant.

```{code-cell}
p = Polynomials.fit(t, y, n)
x = range(-1, 1, length=1000)    # use a lot of points
plot!(x, p.(x), label="interpolant")
```

Surely there must be functions that are more intuitively representative of those points!
``````

(demo-interpolation-pwise-julia)=
``````{dropdown} Piecewise polynomial interpolation
Let us recall the data from {numref}`Demo %s <demo-interpolation-global>`.

```{code-cell}
n = 12
t = range(-1, 1, length=n + 1)
y = @. t^2 + t + 0.5 * sin(20 * t)

scatter(t, y, label="data", leg=:top)
```

Here is an interpolant that is linear between each consecutive pair of nodes, using `plinterp` from {numref}`section-localapprox-pwlin`.

```{code-cell}
p = FNC.plinterp(t, y)
plot!(p, -1, 1, label="piecewise linear")
```

```{index} ! Julia; Spline1D
```

We may prefer a smoother interpolant that is piecewise cubic, generated using `Spline1D` from the `Dierckx` package.

```{code-cell}
p = Spline1D(t, y)
plot!(x -> p(x), -1, 1, label="piecewise cubic")
```
``````

(demo-interp-cond-julia)=
``````{dropdown} Conditioning of interpolation
In {numref}`Demo %s <demo-interpolation-global>` and {numref}`Demo %s <demo-interpolation-pwise>` we saw a big difference between polynomial interpolation and piecewise polynomial interpolation of some arbitrarily chosen data. The same effects can be seen clearly in the cardinal functions, which are closely tied to the condition numbers.

```{code-cell}
n = 18
t = range(-1, stop=1, length=n + 1)
y = [zeros(9); 1; zeros(n - 9)];  # data for 10th cardinal function

scatter(t, y, label="data")
```

```{code-cell}
ϕ = Spline1D(t, y)
plot!(x -> ϕ(x), -1, 1, label="spline",
    xlabel=L"x", ylabel=L"\phi(x)",
    title="Piecewise cubic cardinal function")
```

The piecewise cubic cardinal function is nowhere greater than one in absolute value. This happens to be true for all the cardinal functions, ensuring a good condition number for any interpolation with these functions. But the story for global polynomials is very different.

```{code-cell}
scatter(t, y, label="data")

ϕ = Polynomials.fit(t, y, n)
plot!(x -> ϕ(x), -1, 1, label="polynomial",
    xlabel=L"x", ylabel=L"\phi(x)", legend=:top,
    title="Polynomial cardinal function")
```

From the figure we can see that the condition number for polynomial interpolation on these nodes is at least 500.
``````

(function-hatfun-julia)=
``````{dropdown} Hat function
```{literalinclude} ../julia/package/src/chapter05.jl
:filename: hatfun.jl
:start-line: 0
:end-line: 19
:language: julia
:linenos: true
```
``````

(demo-pwlin-hat-julia)=
``````{dropdown} A look at hat functions
Let's define a set of four nodes (i.e., $n=3$ in our formulas).

```{index} ! Julia; annotate!
```

```{code-cell}
t = [0, 0.55, 0.7, 1]
```

::::{grid} 1 1 2 2
We plot the hat functions $H_0,\ldots,H_3$.
:::{card}
Use `annotate!` to add text to a plot.
:::
::::

```{code-cell}
plt = plot(layout=(4, 1), legend=:top,
    xlabel=L"x", ylims=[-0.1, 1.1], ytick=[])
for k in 0:3
    Hₖ = FNC.hatfun(t, k)
    plot!(Hₖ, 0, 1, subplot=k + 1)
    scatter!(t, Hₖ.(t), m=3, subplot=k + 1)
    annotate!(t[k+1], 0.25, text(latexstring("H_$k"), 10), subplot=k + 1)
end
plt
```
``````

(function-plinterp-julia)=
``````{dropdown} Piecewise linear interpolation
```{literalinclude} ../julia/package/src/chapter05.jl
:filename: plinterp.jl
:start-line: 21
:end-line: 31
:language: julia
:linenos: true
```
``````

(demo-pwlin-usage-julia)=
``````{dropdown} Using piecewise linear interpolation
We generate a piecewise linear interpolant of $f(x)=e^{\sin 7x}$.

```{code-cell}
f = x -> exp(sin(7 * x))

plot(f, 0, 1, label="function", xlabel=L"x", ylabel=L"y")
```

First we sample the function to create the data.

```{code-cell}
t = [0, 0.075, 0.25, 0.55, 0.7, 1]    # nodes
y = f.(t)                             # function values

scatter!(t, y, label="values at nodes")
```

Now we create a callable function that will evaluate the piecewise linear interpolant at any $x$, and then plot it.

```{code-cell}
p = FNC.plinterp(t, y)
plot!(p, 0, 1, label="interpolant", title="PL interpolation")
```
``````

(demo-pwlin-converge-julia)=
``````{dropdown} Convergence of piecewise linear interpolation
We measure the convergence rate for piecewise linear interpolation of $e^{\sin 7x}$ over $x \in [0,1]$.

```{code-cell}
f = x -> exp(sin(7 * x))
x = range(0, 1, length=10001)  # sample the difference at many points
n = @. round(Int, 10^(1:0.25:3.5))
maxerr = zeros(0)
for n in n
    t = (0:n) / n    # interpolation nodes
    p = FNC.plinterp(t, f.(t))
    err = @. f(x) - p(x)
    push!(maxerr, norm(err, Inf))
end

data = (n=n[1:4:end], err=maxerr[1:4:end])
pretty_table(data, header=["n", "max-norm error"])
```

As predicted, a factor of 10 in $n$ produces a factor of 100 in the error. In a convergence plot, it is traditional to have $h$ *decrease* from left to right, so we expect a straight line of slope $-2$ on a log-log plot.

```{code-cell}
h = @. 1 / n
order2 = @. 10 * (h / h[1])^2

plot(h, maxerr, m=:o, label="error")
plot!(h, order2, l=:dash, label=L"O(h^2)", xflip=true,
    xaxis=(:log10, L"h"), yaxis=(:log10, L"|| f-p\, ||_\infty"),
    title="Convergence of PL interpolation")
```
``````

(function-spinterp-julia)=
``````{dropdown} Cubic spline interpolation
```{literalinclude} ../julia/package/src/chapter05.jl
:filename: spinterp.jl
:start-line: 33
:end-line: 92
:language: julia
:linenos: true
```
``````

(demo-splines-splines-julia)=
``````{dropdown} Cubic splines
For illustration, here is a spline interpolant using just a few nodes.

```{code-cell}
f = x -> exp(sin(7 * x))

plot(f, 0, 1, label="function", xlabel=L"x", ylabel=L"y")

t = [0, 0.075, 0.25, 0.55, 0.7, 1]  # nodes
y = f.(t)                           # values at nodes

scatter!(t, y, label="values at nodes")
```

```{code-cell}
S = FNC.spinterp(t, y)

plot!(S, 0, 1, label="spline")
```

Now we look at the convergence rate as the number of nodes increases.

```{code-cell}
x = (0:10000) / 1e4              # sample the difference at many points
n = @. round(Int, 2^(3:0.5:7))  # numbers of nodes
err = zeros(0)
for n in n
    t = (0:n) / n
    S = FNC.spinterp(t, f.(t))
    dif = @. f(x) - S(x)
    push!(err, norm(dif, Inf))
end

pretty_table((; n, err), header=["n", "error"])
```

Since we expect convergence that is $O(h^4)=O(n^{-4})$, we use a log-log graph of error and expect a straight line of slope $-4$.

```{code-cell}
order4 = @. (n / n[1])^(-4)

plot(n, [err order4], m=[:o :none], l=[:solid :dash],
    label=["error" "4th order"],
    xaxis=(:log10, "n"), yaxis=(:log10, L"|| f-S\,||_\infty"),
    title="Convergence of spline interpolation")
```
``````

(demo-finitediffs-fd1-julia)=
``````{dropdown} Finite differences
```{literalinclude} ../julia/package/src/chapter05.jl
If $f(x)=e^{\,\sin(x)}$, then $f'(0)=1$.

```{code-cell}
f = x -> exp(sin(x));
```

Here are the first two centered differences from {numref}`table-FDcenter`.

```{code-cell}
h = 0.05
CD2 = (-f(-h) + f(h)) / 2h
CD4 = (f(-2h) - 8f(-h) + 8f(h) - f(2h)) / 12h
@show (CD2, CD4);
```

Here are the first two forward differences from {numref}`table-FDforward`.

```{code-cell}
FD1 = (-f(0) + f(h)) / h
FD2 = (-3f(0) + 4f(h) - f(2h)) / 2h
@show (FD1, FD2);
```

Finally, here are the backward differences that come from reverse-negating the forward differences.

```{code-cell}
BD1 = (-f(-h) + f(0)) / h
BD2 = (f(-2h) - 4f(-h) + 3f(0)) / 2h
@show (BD1, BD2);
```
``````

(demo-finitediffs-fd2-julia)=
``````{dropdown} Finite differences for $f''$
```{literalinclude} ../julia/package/src/chapter05.jl
If $f(x)=e^{\,\sin(x)}$, then $f''(0)=1$.

```{code-cell}
f = x -> exp(sin(x));
```

Here is a centered estimate given by {eq}`centerFD22`.

```{code-cell}
h = 0.05
CD2 = (f(-h) - 2f(0) + f(h)) / h^2
@show CD2;
```

For the same $h$, here are forward estimates given by {eq}`forwardFD21` and {eq}`forwardFD22`.

```{code-cell}
FD1 = (f(0) - 2f(h) + f(2h)) / h^2
FD2 = (2f(0) - 5f(h) + 4f(2h) - f(3h)) / h^2
@show (FD1, FD2);
```

Finally, here are the backward estimates that come from reversing {eq}`forwardFD21` and {eq}`forwardFD22`.

```{code-cell}
BD1 = (f(-2h) - 2f(-h) + f(0)) / h^2
BD2 = (-f(-3h) + 4f(-2h) - 5f(-h) + 2f(0)) / h^2
@show (BD1, BD2);
```
``````

(function-fdweights-julia)=
``````{dropdown} Fornberg's algorithm for finite difference weights
```{literalinclude} ../julia/package/src/chapter05.jl
:filename: fdweights.jl
:start-line: 94
:end-line: 130
:language: julia
:linenos: true
```
``````

(demo-finitediffs-fd-weights-julia)=
``````{dropdown} Finite differences at arbitrary nodes
We will estimate the derivative of $\cos(x^2)$ at $x=0.5$ using five nodes.

```{code-cell}
t = [0.35, 0.5, 0.57, 0.6, 0.75]   # nodes
f = x -> cos(x^2)
dfdx = x -> -2 * x * sin(x^2)
exact_value = dfdx(0.5)
```

We have to shift the nodes so that the point of estimation for the derivative is at $x=0$. (To subtract a scalar from a vector, we must use the `.-` operator.)

```{code-cell}
w = FNC.fdweights(t .- 0.5, 1)
```

The finite-difference formula is a dot product (i.e., inner product) between the vector of weights and the vector of function values at the nodes.

```{code-cell}
fd_value = dot(w, f.(t))
```

We can reproduce the weights in the finite-difference tables by using equally spaced nodes with $h=1$. For example, here is a one-sided formula at four nodes.

```{code-cell}
FNC.fdweights(0:3, 1)
```

```{index} ! Julia; Rational
```

By giving nodes of type `Rational`, we can get exact values instead.

```{code-cell}
FNC.fdweights(Rational.(0:3), 1)
```
``````


