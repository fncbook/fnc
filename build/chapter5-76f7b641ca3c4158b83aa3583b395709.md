---
kernelspec:
  display_name: Python 3
  language: python
  name: python3
numbering:
  headings: false
---
# Chapter 5

Python implementations

## Functions 

(function-hatfun-python)=
``````{dropdown} Hat function
```{literalinclude} ../python/pkg/FNC/FNC05.py
:filename: hatfun.py
:start-line: 3
:end-line: 25
:language: python
:linenos: true
```
``````

(function-plinterp-python)=
``````{dropdown} Piecewise linear interpolation
```{literalinclude} ../python/pkg/FNC/FNC05.py
:filename: plinterp.py
:start-line: 27
:end-line: 35
:language: python
:linenos: true
```
``````

(function-spinterp-python)=
``````{dropdown} Cubic spline interpolation
```{literalinclude} ../python/pkg/FNC/FNC05.py
:filename: spinterp.py
:start-line: 37
:end-line: 95
:language: python
:linenos: true
```
``````

(function-fdweights-python)=
``````{dropdown} Fornberg's algorithm for finite difference weights
```{literalinclude} ../python/pkg/FNC/FNC05.py
:filename: fdweights.py
:start-line: 97
:end-line: 138
:language: python
:linenos: true
```
``````

(function-trapezoid-python)=
``````{dropdown} Trapezoid formula for numerical integration
```{literalinclude} ../python/pkg/FNC/FNC05.py
:filename: trapezoid.py
:start-line: 140
:end-line: 150
:language: python
:linenos: true
```
``````

(function-intadapt-python)=
``````{dropdown} Adaptive integration
```{literalinclude} ../python/pkg/FNC/FNC05.py
:filename: intadapt.py
:start-line: 152
:end-line: 191
:language: python
:linenos: true
```
:::{admonition} About the code
:class: dropdown
The intended way for a user to call {numref}`Function {number} <function-intadapt>` is with only `f`, `a`, `b`, and `tol` provided. We then use default values on the other parameters to compute the function values at the endpoints, the interval's midpoint, and the function value at the midpoint. Recursive calls from within the function itself will provide all of that information, since it was already calculated along the way.
:::
``````

## Examples

```{code-cell} ipython3
import FNC
from numpy import *
from matplotlib.pyplot import *
from numpy.linalg import solve, norm
from scipy.interpolate import interp1d
from scipy.integrate import quadrature
from prettytable import PrettyTable
```

```{code-cell} ipython3
:tags: [remove-cell]
# This (optional) block is for improving the display of plots.
rcParams["figure.figsize"] = [7, 4]
rcParams["lines.linewidth"] = 2
rcParams["lines.markersize"] = 4
rcParams['animation.html'] = "jshtml"  # or try "html5"
``` 


### Section 5.1

(demo-interpolation-global-python)=
``````{dropdown} Trouble in polynomial interpolation
Here are some points that we could consider to be observations of an unknown function on $[-1,1]$.

```{code-cell}
n = 5
t = linspace(-1, 1, n + 1)
y = t**2 + t + 0.05 * sin(20 * t)
fig, ax = subplots()
plot(t, y, "o", label="data")
xlabel("$x$")
ylabel("$y$")
```

```{index} ! Julia; fit
```

The polynomial interpolant, as computed using `fit`, looks very sensible. It's the kind of function you'd take home to meet your parents.

```{code-cell}
p = poly1d(polyfit(t, y, n))  # interpolating polynomial
tt = linspace(-1, 1, 400)
ax.plot(tt, p(tt), label="interpolant")
ax.legend()
fig
```

But now consider a different set of points generated in almost exactly the same way.

```{code-cell}
n = 18
t = linspace(-1, 1, n + 1)
y = t**2 + t + 0.05 * sin(20 * t)
fig, ax = subplots()
plot(t, y, "o", label="data")
xlabel("$x$")
ylabel("$y$")
```

The points themselves are unremarkable. But take a look at what happens to the polynomial interpolant.

```{code-cell}
p = poly1d(polyfit(t, y, n))
ax.plot(tt, p(tt), label="interpolant")
ax.legend()
fig
```

Surely there must be functions that are more intuitively representative of those points!
``````

(demo-interpolation-pwise-python)=
``````{dropdown} Piecewise polynomial interpolation
Let us recall the data from {numref}`Demo %s <demo-interpolation-global>`.

```{code-cell}
clf
n = 12
t = linspace(-1, 1, n + 1)
y = t**2 + t + 0.5 * sin(20 * t)
fig, ax = subplots()
plot(t, y, "o", label="data")
xlabel("$x$")
ylabel("$y$")
```

Here is an interpolant that is linear between each consecutive pair of nodes, using `plinterp` from {numref}`section-localapprox-pwlin`.

```{code-cell}
tt = linspace(-1, 1, 400)
p = interp1d(t, y, kind="linear")
ax.plot(tt, p(tt), label="piecewise linear")
ax.legend()
fig
```

```{index} ! Julia; Spline1D
```

We may prefer a smoother interpolant that is piecewise cubic:

```{code-cell}
plot(t, y, "o", label="data")
p = interp1d(t, y, kind="cubic")
tt = linspace(-1, 1, 400)
plot(tt, p(tt), label="cubic spline")
xlabel("$x$")
ylabel("$y$")
legend()
```
``````

(demo-interp-cond-python)=
``````{dropdown} Conditioning of interpolation
In {numref}`Demo %s <demo-interpolation-global>` and {numref}`Demo %s <demo-interpolation-pwise>` we saw a big difference between polynomial interpolation and piecewise polynomial interpolation of some arbitrarily chosen data. The same effects can be seen clearly in the cardinal functions, which are closely tied to the condition numbers.

```{code-cell}
clf
n = 18
t = linspace(-1, 1, n + 1)
y = zeros(n + 1)
y[9] = 1.0
scatter(t, y, label="data")

p = interp1d(t, y, kind="cubic")
tt = linspace(-1, 1, 400)
plot(tt, p(tt))
title("Cubic spline cardinal function")
```

The piecewise cubic cardinal function is nowhere greater than one in absolute value. This happens to be true for all the cardinal functions, ensuring a good condition number for any interpolation with these functions. But the story for global polynomials is very different.

```{code-cell}
scatter(t, y, label="data")
p = poly1d(polyfit(t, y, n))
plot(tt, p(tt), label="polynomial")
xlabel("$x$")
ylabel("$y$")
title("Polynomial cardinal function")
```

From the figure we can see that the condition number for polynomial interpolation on these nodes is at least 500.
``````

### Section 5.2

(demo-pwlin-hat-python)=
``````{dropdown} A look at hat functions
Let's define a set of four nodes (i.e., $n=3$ in our formulas).

```{index} ! Julia; annotate!
```

```{code-cell}
t = array([0, 0.075, 0.25, 0.55, 0.7, 1])
```

We plot the hat functions $H_0,\ldots,H_3$.

```{code-cell}
x = linspace(0, 1, 300)
for k in range(6):
    plot(x, FNC.hatfun(x, t, k))
xlabel("$x$")
ylabel("$H_k(x)$")
title("Hat functions")
```
``````

(demo-pwlin-usage-python)=
``````{dropdown} Using piecewise linear interpolation
We generate a piecewise linear interpolant of $f(x)=e^{\sin 7x}$.

```{code-cell}
f = lambda x: exp(sin(7 * x))
x = linspace(0, 1, 400)
fig, ax = subplots()
plot(x, f(x), label="function")
xlabel("$x$")
ylabel("$f(x)$")
```

First we sample the function to create the data.

```{code-cell}
t = array([0, 0.075, 0.25, 0.55, 0.7, 1])  # nodes
y = f(t)  # function values

ax.plot(t, y, "o", label="nodes")
ax.legend()
fig
```

Now we create a callable function that will evaluate the piecewise linear interpolant at any $x$, and then plot it.

```{code-cell}
p = FNC.plinterp(t, y)
ax.plot(x, p(x), label="interpolant")
ax.legend()
fig
```
``````

(demo-pwlin-converge-python)=
``````{dropdown} Convergence of piecewise linear interpolation
We measure the convergence rate for piecewise linear interpolation of $e^{\sin 7x}$ over $x \in [0,1]$.

```{code-cell}
f = lambda x: exp(sin(7 * x))
x = linspace(0, 1, 10000)  # sample the difference at many points
N = 2 ** arange(3, 11)
err = zeros(N.size)
for i, n in enumerate(N):
    t = linspace(0, 1, n + 1)  # interpolation nodes
    p = FNC.plinterp(t, f(t))
    err[i] = max(abs(f(x) - p(x)))
print(err)
```

As predicted, a factor of 10 in $n$ produces a factor of 100 in the error. In a convergence plot, it is traditional to have $h$ *decrease* from left to right, so we expect a straight line of slope $-2$ on a log-log plot.

```{code-cell}
order2 = 0.1 * (N / N[0]) ** (-2)
loglog(N, err, "-o", label="observed error")
loglog(N, order2, "--", label="2nd order")
xlabel("$n$")
ylabel("$\|f-p\|_\infty$")
legend()
```
``````

### Section 5.3 

(demo-splines-splines-python)=
``````{dropdown} Cubic splines
For illustration, here is a spline interpolant using just a few nodes.

```{code-cell}
f = lambda x: exp(sin(7 * x))

x = linspace(0, 1, 500)
fig, ax = subplots()
ax.plot(x, f(x), label="function")

t = array([0, 0.075, 0.25, 0.55, 0.7, 1])  # nodes
y = f(t)  # values at nodes

xlabel("$x$")
ylabel("$y$")
ax.scatter(t, y, label="nodes")
```

```{code-cell}
S = FNC.spinterp(t, y)
ax.plot(x, S(x), label="spline")
ax.legend()
fig
```

Now we look at the convergence rate as the number of nodes increases.

```{code-cell}
N = floor(2 ** linspace(3, 8, 17)).astype(int)
err = zeros(N.size)
for i, n in enumerate(N):
    t = linspace(0, 1, n + 1)  # interpolation nodes
    p = FNC.spinterp(t, f(t))
    err[i] = max(abs(f(x) - p(x)))
print(err)
```

Since we expect convergence that is $O(h^4)=O(n^{-4})$, we use a log-log graph of error and expect a straight line of slope $-4$.

```{code-cell}
order4 = (N / N[0]) ** (-4)
loglog(N, err, "-o", label="observed error")
loglog(N, order4, "--", label="4th order")
xlabel("$n$")
ylabel("$\|f-S\|_\infty$")
legend()
```
``````

### Section 5.4

(demo-finitediffs-fd1-python)=
``````{dropdown} Finite differences
If $f(x)=e^{\,\sin(x)}$, then $f'(0)=1$.

```{code-cell}
f = lambda x: exp(sin(x))
```

Here are the first two centered differences from {numref}`table-FDcenter`.

```{code-cell}
h = 0.05
CD2 = (-f(-h) + f(h)) / (2*h)
CD4 = (f(-2h) - 8*f(-h) + 8*f(h) - f(2*h)) / (12*h)
print(f"CD2 is {CD2:.6f} and CD4 is {CD4:.6f}")
```

Here are the first two forward differences from {numref}`table-FDforward`.

```{code-cell}
FD1 = (-f(0) + f(h)) / h
FD2 = (-3f(0) + 4*f(h) - f(2*h)) / (2*h)
print(f"FD1 is {FD1:.6f} and FD2 is {FD2:.6f}")
```

Finally, here are the backward differences that come from reverse-negating the forward differences.

```{code-cell}
BD1 = (-f(-h) + f(0)) / h
BD2 = (f(-2*h) - 4*f(-h) + 3*f(0)) / (2*h)
print(f"BD1 is {BD1:.6f} and BD2 is {BD2:.6f}")
```
``````

(demo-finitediffs-fd2-python)=
``````{dropdown} Finite differences for $f''$
If $f(x)=e^{\,\sin(x)}$, then $f''(0)=1$.

```{code-cell}
f = lambda x: exp(sin(x))
```

Here is a centered estimate given by {eq}`centerFD22`.

```{code-cell}
h = 0.05
CD2 = (f(-h) - 2*f(0) + f(h)) / h**2
print(f"CD2 is {CD2:.6f}")
```

For the same $h$, here are forward estimates given by {eq}`forwardFD21` and {eq}`forwardFD22`.

```{code-cell}
FD1 = (f(0) - 2*f(h) + f(2*h)) / h**2
FD2 = (2f(0) - 5*f(h) + 4*f(2*h) - f(3*h)) / h**2
print(f"FD1 is {FD1:.6f} and FD2 is {FD2:.6f}")
```

Finally, here are the backward estimates that come from reversing {eq}`forwardFD21` and {eq}`forwardFD22`.

```{code-cell}
BD1 = (f(-2*h) - 2*f(-h) + f(0)) / h**2
BD2 = (-f(-3*h) + 4*f(-2*h) - 5*f(-h) + 2f(0)) / h**2
print(f"BD1 is {BD1:.6f} and BD2 is {BD2:.6f}")
```
``````

(demo-finitediffs-fd-weights-python)=
``````{dropdown} Finite differences at arbitrary nodes
We will estimate the derivative of $\cos(x^2)$ at $x=0.5$ using five nodes.

```{code-cell}
t = array([0.35, 0.5, 0.57, 0.6, 0.75])   # nodes
f = lambda x: cos(x**2)
dfdx = lambda x: -2 * x * sin(x**2)
exact_value = dfdx(0.5)
```

We have to shift the nodes so that the point of estimation for the derivative is at $x=0$. (To subtract a scalar from a vector, we must use the `.-` operator.)

```{code-cell}
w = FNC.fdweights(t - 0.5, 1)
```

The finite-difference formula is a dot product (i.e., inner product) between the vector of weights and the vector of function values at the nodes.

```{code-cell}
fd_value = dot(w, f(t))
```

We can reproduce the weights in the finite-difference tables by using equally spaced nodes with $h=1$. For example, here is a one-sided formula at four nodes.

```{code-cell}
FNC.fdweights(0:3, 1)
```
``````

### Section 5.5

(demo-fdconverge-order12-python)=
``````{dropdown} Convergence of finite differences
Let's observe the convergence of the formulas in {numref}`Example {number} <example-fd-converge-FD11>` and {numref}`Example {number} <example-fd-converge-FD12>`, applied to the function $\sin(e^{x+1})$ at $x=0$.

```{code-cell}
f = lambda x: sin(exp(x + 1))
exact_value = exp(1) * cos(exp(1))
```

We'll compute the formulas in parallel for a sequence of $h$ values.

```{code-cell}
h_ = [5 / 10**(n+1) for n in range(6)]
FD = zeros(len(h_), 2)
for (i, h) in enumerate(h_):
    FD[i, 0] = (f(h) - f(0)) / h 
    FD[i, 1] = (f(h) - f(-h)) / (2*h)
results = PrettyTable()
results.add_column("h", h_)
results.add_column("FD1", FD[:, 0])
results.add_column("FD2", FD[:, 1])
print(results)
```

All that's easy to see from this table is that FD2 appears to converge to the same result as FD1, but more rapidly. A table of errors is more informative.

```{code-cell}
errors = FD - exact_value
results = PrettyTable()
results.add_column("h", h_)
results.add_column("error in FD1", error[:, 0])
results.add_column("error in FD2", error[:, 1])
print(results)
```

In each row, $h$ is decreased by a factor of 10, so that the error is reduced by a factor of 10 in the first-order method and 100 in the second-order method.

A graphical comparison can be useful as well. On a log-log scale, the error should (as $h\to 0$) be a straight line whose slope is the order of accuracy. However, it's conventional in convergence plots to show $h$ _decreasing_ from left to right, which negates the slopes.

```{code-cell}
plot(h_, abs(errors), "o-", label=["FD1", "FD2"])
# Add lines for perfect 1st and 2nd order.
plot(h_, hstack([h, h**2]), "--", label=["$O(h)$", "$O(h^2)$"])
legend()
```
``````

(demo-fdconverge-round-python)=
``````{dropdown} Roundoff error in finite differences
Let $f(x)=e^{-1.3x}$. We apply finite-difference formulas of first, second, and fourth order to estimate $f'(0)=-1.3$.

```{code-cell}
f = x -> exp(-1.3 * x);
exact = -1.3

h = [1 / 10^n for n in 1:12]
FD1, FD2, FD4 = [], [], []
for h in h
    nodes = h * (-2:2)
    vals = @. f(nodes)
    push!(FD1, dot([0 0 -1 1 0] / h, vals))
    push!(FD2, dot([0 -1 / 2 0 1 / 2 0] / h, vals))
    push!(FD4, dot([1 / 12 -2 / 3 0 2 / 3 -1 / 12] / h, vals))
end

table = [h FD1 FD2 FD4]
pretty_table(table[1:4, :], header=["h", "FD1", "FD2", "FD4"])
```

They all seem to be converging to $-1.3$. The convergence plot reveals some interesting structure to the errors, though.

```{code-cell}
err = @. abs([FD1 FD2 FD4] - exact)

plot(h, err, m=:o, label=["FD1" "FD2" "FD4"],
    xaxis=(:log10, L"h"), xflip=true, yaxis=(:log10, "error"),
    title="FD error with roundoff", legend=:bottomright)

# Add line for perfect 1st order.
plot!(h, 0.1 * eps() ./ h, l=:dash, color=:black, label=L"O(h^{-1})")
```

Again the graph is made so that $h$ decreases from left to right. The errors are dominated at first by truncation error, which decreases most rapidly for the fourth-order formula. However, increasing roundoff error eventually equals and then dominates the truncation error as $h$ continues to decrease. As the order of accuracy increases, the crossover point moves to the left (greater efficiency) and down (greater accuracy).
``````

### Section 5.6
(demo-int-antideriv-python)=
``````{dropdown} Numerical integration
The antiderivative of $e^x$ is, of course, itself. That makes evaluation of $\int_0^1 e^x\,dx$ by the Fundamental Theorem trivial.

```{code-cell}
exact = exp(1) - 1
```

```{index} ! Julia; quadgk
```

The Julia package `QuadGK` has an all-purpose numerical integrator that estimates the value without finding the antiderivative first. As you can see here, it's often just as accurate.

```{code-cell}
Q, errest = quadgk(x -> exp(x), 0, 1)
@show Q;
```

The numerical approach is also far more robust. For example, $e^{\,\sin x}$ has no useful antiderivative. But numerically, it's no more difficult.

```{code-cell}
Q, errest = quadgk(x -> exp(sin(x)), 0, 1)
@show Q;
```

When you look at the graphs of these functions, what's remarkable is that one of these areas is basic calculus while the other is almost impenetrable analytically. From a numerical standpoint, they are practically the same problem.

```{code-cell}
plot([exp, x -> exp(sin(x))], 0, 1, fill=0, layout=(2, 1),
    xlabel=L"x", ylabel=[L"e^x" L"e^{\sin(x)}"], ylim=[0, 2.7])
```
``````

(demo-int-trap-python)=
``````{dropdown} Trapezoid integration
We will approximate the integral of the function $f(x)=e^{\sin 7x}$ over the interval $[0,2]$.

```{code-cell}
f = x -> exp(sin(7 * x));
a = 0;
b = 2;
```

::::{grid} 1 1 2 2
In lieu of the exact value, we use the `QuadGK` package to find an accurate result.
:::
:::{card}
If a function has multiple return values, you can use an underscore `_` to indicate a  return value you want to ignore.
:::
::::

```{code-cell}
Q, _ = quadgk(f, a, b, atol=1e-14, rtol=1e-14);
println("Integral = $Q")
```

Here is the trapezoid result at $n=40$, and its error.

```{code-cell}
T, t, y = FNC.trapezoid(f, a, b, 40)
@show (T, Q - T);
```

In order to check the order of accuracy, we increase $n$ by orders of magnitude and observe how the error decreases.

```{code-cell}
n = [10^n for n in 1:5]
err = []
for n in n
    T, t, y = FNC.trapezoid(f, a, b, n)
    push!(err, Q - T)
end

pretty_table([n err], header=["n", "error"])
```

Each increase by a factor of 10 in $n$ cuts the error by a factor of about 100, which is consistent with second-order convergence. Another check is that a log-log graph should give a line of slope $-2$ as $n\to\infty$.

```{code-cell}
plot(n, abs.(err), m=:o, label="results",
    xaxis=(:log10, L"n"), yaxis=(:log10, "error"),
    title="Convergence of trapezoidal integration")

# Add line for perfect 2nd order.
plot!(n, 3e-3 * (n / n[1]) .^ (-2), l=:dash, label=L"O(n^{-2})")
```
``````

(demo-int-extrap-python)=
``````{dropdown} Integration by extrapolation
We estimate $\displaystyle\int_0^2 x^2 e^{-2x}\, dx$ using extrapolation. First we use `quadgk` to get an accurate value.

```{code-cell}
f = x -> x^2 * exp(-2 * x);
a = 0;
b = 2;
Q, _ = quadgk(f, a, b, atol=1e-14, rtol=1e-14)
@show Q;
```

We start with the trapezoid formula on $n=N$ nodes.

```{code-cell}
N = 20;       # the coarsest formula
n = N;
h = (b - a) / n;
t = h * (0:n);
y = f.(t);
```

We can now apply weights to get the estimate $T_f(N)$.

```{code-cell}
T = [h * (sum(y[2:n]) + y[1] / 2 + y[n+1] / 2)]
```

Now we double to $n=2N$, but we only need to evaluate $f$ at every other interior node and apply {eq}`nc-doubling`.

```{code-cell}
n = 2n;
h = h / 2;
t = h * (0:n);
T = [T; T[end] / 2 + h * sum(f.(t[2:2:n]))]
```

We can repeat the same code to double $n$ again.

```{code-cell}
n = 2n;
h = h / 2;
t = h * (0:n);
T = [T; T[end] / 2 + h * sum(f.(t[2:2:n]))]
```

Let us now do the first level of extrapolation to get results from Simpson's formula. We combine the elements `T[i]` and `T[i+1]` the same way for $i=1$ and $i=2$.

```{code-cell}
S = [(4T[i+1] - T[i]) / 3 for i in 1:2]
```

With the two Simpson values $S_f(N)$ and $S_f(2N)$ in hand, we can do one more level of extrapolation to get a sixth-order accurate result.

```{code-cell}
R = (16S[2] - S[1]) / 15
```

::::{grid} 1 1 2 2
We can make a triangular table of the errors:
:::{card}
The value `nothing` equals nothing except `nothing`.
:::
::::

```{code-cell}
err = [T .- Q [nothing; S .- Q] [nothing; nothing; R - Q]]
pretty_table(err, header=["order 2", "order 4", "order 6"])
```

If we consider the computational time to be dominated by evaluations of $f$, then we have obtained a result with about twice as many accurate digits as the best trapezoid result, at virtually no extra cost.
``````

### Section 5.7

(demo-adapt-motive-python)=
``````{dropdown} Motivation for adaptive integration
This function gets increasingly oscillatory as $x$ increases.

```{code-cell}
f = x -> (x + 1)^2 * cos((2 * x + 1) / (x - 4.3))
plot(f, 0, 4, xlabel=L"x", ylabel=L"f(x)")
```

Accordingly, the trapezoid rule is more accurate on the left half of this interval than on the right half.

```{code-cell}
left_val, _ = quadgk(f, 0, 2, atol=1e-14, rtol=1e-14)
right_val, _ = quadgk(f, 2, 4, atol=1e-14, rtol=1e-14)

n = [50 * 2^k for k in 0:3]
left_err, right_err = [], []
for n in n
    T, _ = FNC.trapezoid(f, 0, 2, n)
    push!(left_err, T - left_val)

    T, _ = FNC.trapezoid(f, 2, 4, n)
    push!(right_err, T - right_val)
end

pretty_table([n left_err right_err], header=["n", "left error", "right error"])
```

Both the picture and the numerical results suggest that more nodes should be used on the right half of the interval than on the left half.
``````

(demo-adapt-usage-python)=
``````{dropdown} Using adaptive integration
We'll integrate the function from {numref}`Demo %s <demo-adapt-motive>`.

```{code-cell}
f = x -> (x + 1)^2 * cos((2 * x + 1) / (x - 4.3));
```

We perform the integration and show the nodes selected underneath the curve.

```{code-cell}
A, t = FNC.intadapt(f, 0, 4, 0.001)
@show num_nodes = length(t);

plot(f, 0, 4, color=:black, legend=:none,
    xlabel=L"x", ylabel=L"f(x)", title="Adaptive node selection")
plot!(t, f.(t), seriestype=:sticks, m=(:o, 2))
```

The error turns out to be a bit more than we requested. It's only an estimate, not a guarantee.

```{code-cell}
Q, _ = quadgk(f, 0, 4, atol=1e-14, rtol=1e-14);    # 'exact' value
println("error: $(Q-A)");
```

Let's see how the number of integrand evaluations and the error vary with the requested tolerance.

```{code-cell}
tol = [1 / 10^k for k in 4:14]
err, n = [], []
for tol in 10.0 .^ (-4:-1:-14)
    A, t = FNC.intadapt(f, 0, 4, tol)
    push!(err, Q - A)
    push!(n, length(t))
end

pretty_table([tol err n], header=["tolerance", "error", "number of nodes"])
```

As you can see, even though the errors are not smaller than the estimates, the two columns decrease in tandem. If we consider now the convergence not in $h$, which is poorly defined now, but in the number of nodes actually chosen, we come close to the fourth-order accuracy of the underlying Simpson scheme.

```{code-cell}
plot(n, abs.(err), m=:o, label="results",
    xaxis=(:log10, "number of nodes"), yaxis=(:log10, "error"),
    title="Convergence of adaptive integration")

order4 = @. 0.01 * (n / n[1])^(-4)
plot!(n, order4, l=:dash, label=L"O(n^{-4})")
```
``````
