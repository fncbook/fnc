# Glossary

:::{glossary}
adjacency matrix
: Matrix whose nonzero entries show the links between nodes in a graph. \
(@section-matrixanaly-insight)

adjoint
: Conjugate transpose of a complex matrix. \
({numref}`§ {number} <section-linsys-matrices>`)</br>

advection equation
: Archetypical PDE of hyperbolic type, representing transport phenomena. \
({numref}`§ {number} <section-advection-traffic>`)</br>

algorithm
: List of instructions for transforming data into a result. \
({numref}`§ {number} <section-intro-algorithms>`)</br>

Arnoldi iteration
: Stable algorithm for finding orthonormal bases of nested Krylov subspaces. \
({numref}`§ {number} <section-krylov-subspace>`)</br>

asymptotic
: Relationship indicating that two functions have the same leading behavior in some limit. \
({numref}`§ {number} <section-linsys-efficiency>`)</br>

backward error
: Change to the input of a problem required to produce the result found by an inexact algorithm. \
({numref}`§ {number} <section-intro-stability>`)</br>

backward substitution
: Systematic method for solving a linear system with an upper triangular matrix. \
({numref}`§ {number} <section-linsys-linear-systems>`)</br>

bandwidth
: The number of diagonals around the main diagonal that have nonzero elements. \
({numref}`§ {number} <section-linsys-structure>`)</br>

barycentric formula
: Computationally useful expression for the interpolating polynomial as a ratio of rational terms. \
({numref}`§ {number} <section-globalapprox-barycentric>`)</br>

big-O
: Relationship indicating that one function is bounded above by a multiple of another in some limit. \
({numref}`§ {number} <section-linsys-efficiency>`)</br>

boundary-value problem
: A differential equation with which partial information about the solution is given at multiple points on the boundary of the domain. \
({numref}`§ {number} <section-bvp-tpbvp>`)</br>

cardinal function
: Interpolating function that is 1 at one node and 0 at all the others. \
({numref}`§ {number} <section-localapprox-interpolation>`)</br>

Cholesky factorization
: Symmetrized version of LU factorization for SPD matrices. \
({numref}`§ {number} <section-linsys-structure>`)</br>

collocation
: Solution of a differential equation by imposing it approximately at a set of nodes. \
({numref}`§ {number} <section-bvp-linear>`)</br>

condition number
: Ratio of the size of change in the output of a function to the size of change in the input that produced it. \
({numref}`§ {number} <section-intro-conditioning>`)</br>

cubic spline
: Piecewise cubic function with two globally continuous derivatives, most often used for interpolation or approximation. \
({numref}`§ {number} <section-localapprox-splines>`)</br>

diagonalizable
: Matrix that admits an eigenvalue decomposition. Also known as nondefective. \
({numref}`§ {number} <section-matrixanaly-evd>`)</br>

differentiation matrix
: Matrix mapping a vector of function values to a vector of approximate derivative values. \
({numref}`§ {number} <section-bvp-diffmats>`)</br>

Dirichlet condition
: Boundary condition specifying the value of the solution. \
({numref}`§ {number} <section-bvp-tpbvp>`)</br>

dominant eigenvalue
: Eigenvalue with the largest modulus (absolute value, in the real case). \
({numref}`§ {number} <section-krylov-power>`)</br>

double precision
: Typical standard in floating-point representation, using 64 bits to achieve about 16 decimal significant digits of precision. \
({numref}`§ {number} <section-intro-floating-point>`)</br>

eigenvalue
: Scalar $\lambda$ such that $\mathbf{A}\mathbf{x} = \lambda \mathbf{x}$ for a square matrix $\mathbf{A}$ and nonzero vector $\mathbf{x}$. \
({numref}`§ {number} <section-matrixanaly-evd>`)</br>

eigenvalue decomposition (EVD)
: Expression of a square matrix as the product of eigenvector and diagonal eigenvalue matrices. \
({numref}`§ {number} <section-matrixanaly-evd>`)</br>

eigenvector
: Vector for which the action of a matrix is effectively one-dimensional.  \
({numref}`§ {number} <section-matrixanaly-evd>`)</br>

Euler's method
: Prototype of all IVP solution methods, obtained by assuming constant derivatives for the solution over short time intervals. \
({numref}`§ {number} <section-ivp-euler>`)</br>

evolutionary PDE
: A partial differential equation in which one of the independent variables is time or a close analog. \
({numref}`§ {number} <section-diffusion-blackscholes>`)</br>

extrapolation
: Use of multiple discretization values to cancel out leading terms in an error expansion. \
({numref}`§ {number} <section-localapprox-integration>`)</br>

finite difference
: Linear combination of function values that approximates the value of a derivative of the function at a point. \
({numref}`§ {number} <section-localapprox-finitediffs>`)</br>

finite element method (FEM)
: Use of piecewise integration to pose a linear system of equations for the approximate solution of a boundary-value problem. \
({numref}`§ {number} <section-bvp-galerkin>`)</br>

fixed point iteration
: Repeated application of a function in hopes of converging to a fixed point. \
({numref}`§ {number} <section-nonlineqn-fixed-point>`)</br>

fixed point problem
: Finding a value of a given function where the input and output values are the same; equivalent to rootfinding. \
({numref}`§ {number} <section-nonlineqn-fixed-point>`)</br>

floating-point numbers
: A finite set that substitutes for the real numbers in machine calculations. Denoted by $\mathbb{F}$. \
({numref}`§ {number} <section-intro-floating-point>`)</br>

flops
: Arithmetic operations on floating-point numbers, often counted as a proxy for computer runtime. \
({numref}`§ {number} <section-linsys-efficiency>`)</br>

forward substitution
: Systematic method for solving a linear system with a lower triangular matrix. \
({numref}`§ {number} <section-linsys-linear-systems>`)</br>

Frobenius norm
: Matrix norm computed by applying the vector 2-norm to a vector interpretation of the matrix. \
({numref}`§ {number} <section-linsys-norms>`)</br>

Gauss–Newton method
: Generalization of Newton's method for nonlinear least squares. \
({numref}`§ {number} <section-nonlineqn-nlsq>`)</br>

Gaussian elimination
: Use of row operations to transform a linear system to an equivalent one in triangular form. \
({numref}`§ {number} <section-linsys-lu>`)</br>

generating polynomials
: A pair of polynomials whose coefficients match those of a multistep method for IVPs. \
({numref}`§ {number} <section-ivp-multistep>`)</br>

global error
: Error made by an IVP method over the entire time interval of the solution. \
({numref}`§ {number} <section-ivp-euler>`)</br>

GMRES
: Iterative solution of a linear system through stable least-squares solutions on nested Krylov subspaces. \
({numref}`§ {number} <section-krylov-gmres>`)</br>

graph
: Representation of a network as a set of nodes and edges. \
({numref}`§ {number} <section-matrixanaly-insight>`)</br>

hat functions
: Cardinal functions for piecewise linear interpolation. \
({numref}`§ {number} <section-localapprox-pwlin>`)</br>

heat equation
: Archetypical parabolic PDE that describes diffusion. \
({numref}`§ {number} <section-diffusion-blackscholes>`)</br>

hermitian
: Combination of transpose and elementwise complex conjugation. Also describes a matrix that equals its own hermitian. \
({numref}`§ {number} <section-matrixanaly-symm-eig>`)</br>

hermitian positive definite (HPD)
: Matrix that is hermitian with strictly positive eigenvalues; complex variant of symmetric positive definite. \
({numref}`§ {number} <section-matrixanaly-symm-eig>`)</br>

identity matrix
: Matrix with ones on the diagonal and zeros elsewhere, acting as the multiplicative identity. \
({numref}`§ {number} <section-linsys-matrices>`)</br>

ill-conditioned
: Exhibiting a large condition number, indicating high sensitivity of a result to changes in the data. \
({numref}`§ {number} <section-intro-conditioning>`)</br>

implicit
: Formula that defines a quantity only indirectly, e.g., as the solution of a nonlinear equation. \
({numref}`§ {number} <section-ivp-multistep>`)</br>

induced matrix norm
: Norm computed using the interpretation of a matrix as a linear operator. \
({numref}`§ {number} <section-linsys-norms>`)</br>

initial-value problem (IVP)
: An ordinary differential equation (possibly vector-valued) together with an initial condition. \
({numref}`§ {number} <section-ivp-basics>`, {numref}`§ {number} <section-ivp-systems>`)</br>

inner product
: Scalar or dot product of a pair of vectors, or its extension to a pair of functions. \
({numref}`§ {number} <section-globalapprox-orthogonal>`)</br>

interpolation
: Construction of a function that passes through a given set of data points. \
({numref}`§ {number} <section-linsys-polyinterp>`, {numref}`§ {number} <section-localapprox-interpolation>`)</br>

inverse iteration
: Subtraction of a shift followed by matrix inversion, used in power iteration to transform the eigenvalue closest to a target value into a dominant one.  \
({numref}`§ {number} <section-krylov-inviter>`)</br>

Jacobian matrix
: Matrix of first partial derivatives that defines the linearization of a vector-valued function. \
({numref}`§ {number} <section-nonlineqn-newtonsys>`)</br>

Kronecker product
: Alternative type of matrix multiplication useful for problems on a tensor-product domain. \
({numref}`§ {number} <section-twodim-laplace>`)</br>

Krylov subspace
: Vector space generated by powers of a square matrix that is often useful for reducing the dimension of large problems. \
({numref}`§ {number} <section-krylov-subspace>`)</br>

Lagrange formula
: Theoretically useful expression for an interpolating polynomial. \
({numref}`§ {number} <section-globalapprox-polynomial>`)</br>

Lanczos iteration
: Specialization of the Arnoldi iteration to the case of a hermitian (or real symmetric) matrix. \
({numref}`§ {number} <section-krylov-minrescg>`)</br>

Laplace equation
: Archetypical elliptic PDE describing a steady state. \
({numref}`§ {number} <section-twodim-laplace>`)</br>

linear convergence
: Sequence in which the difference between sequence value and limit asymptotically decreases by a constant factor at each term, making a straight line on a log-linear graph.\ 
({numref}`§ {number} <section-nonlineqn-fixed-point>`)</br>

linear least-squares problem
: Minimization of the 2-norm of the residual for an overdetermined linear system. \
({numref}`§ {number} <section-leastsq-fitting>`)</br>

local truncation error
: Discretization error made in one time step of an IVP solution method. \
({numref}`§ {number} <section-ivp-euler>`, {numref}`§ {number} <section-ivp-multistep>`)</br>

LU factorization
: Factorization of a square matrix into the product of a unit lower triangular matrix and an upper triangular matrix. \
({numref}`§ {number} <section-linsys-lu>`)</br>

machine epsilon
: Distance from 1 to the next-largest floating-point number. Also called unit roundoff or machine precision, though the usages are not consistent across different references.\ 
({numref}`§ {number} <section-intro-floating-point>`)</br>

matrix condition number
: Norm of the matrix times the norm of its inverse, equivalent to the condition number for solving a linear system. \
({numref}`§ {number} <section-linsys-condition-number>`)</br>

method of lines
: Solution technique for partial differential equations in which each independent variable is discretized separately. \
({numref}`§ {number} <section-diffusion-methodlines>`)</br>

multistep
: Formula using information over more than a single time step to advance the solution. \
({numref}`§ {number} <section-ivp-multistep>`)</br>

Neumann condition
: Boundary condition specifying the derivative of the solution. \
({numref}`§ {number} <section-bvp-tpbvp>`)</br>

Newton's method
: Rootfinding iteration that uses the linearization of the given function in order to define the next root approximation. \
({numref}`§ {number} <section-nonlineqn-newton>`)</br>

nodes
: Values of the independent variable where an interpolant's values are prescribed. \
({numref}`§ {number} <section-localapprox-interpolation>`)</br>

nonlinear least-squares problem
: Minimization of the 2-norm of the residual of a function that depends nonlinearly on a vector. \
({numref}`§ {number} <section-nonlineqn-nlsq>`)</br>

norm
: Means of defining the magnitude of a vector or matrix. \
({numref}`§ {number} <section-linsys-norms>`)</br>

normal
: Matrix that has a unitary eigenvalue decomposition. \
({numref}`§ {number} <section-matrixanaly-evd>`)</br>

normal equations
: Square linear system equivalent to the linear least-squares problem. \
({numref}`§ {number} <section-leastsq-normaleqns>`)</br>

numerical integration
: Estimation of a definite integral by combining values of the integrand, rather than by finding an antiderivative. \
({numref}`§ {number} <section-localapprox-integration>`)</br>

one-step IVP method
: IVP solver that uses information from just one time level to advance to the next. \
({numref}`§ {number} <section-ivp-euler>`)</br>

ONC matrix
: Matrix whose columns are orthonormal vectors. \
({numref}`§ {number} <section-leastsq-qr>`)</br>

order of accuracy
: Leading power of the truncation error as a function of a discretization size parameter. \
({numref}`§ {number} <section-localapprox-fd-converge>`, {numref}`§ {number} <section-localapprox-integration>`, {numref}`§ {number} <section-ivp-euler>`, {numref}`§ {number} <section-ivp-multistep>`)</br>

orthogonal vectors
: Nonzero vectors that have an inner product of zero. \
({numref}`§ {number} <section-leastsq-qr>`)</br>

orthogonal matrix
: Square ONC matrix, i.e., matrix whose transpose is its inverse. \
({numref}`§ {number} <section-leastsq-qr>`)</br>

orthogonal polynomials
: Family of polynomials whose distinct members have an integral inner product equal to zero, as with Legendre and Chebyshev polynomials. \
({numref}`§ {number} <section-globalapprox-orthogonal>`)</br>

orthonormal vectors
: Vectors that are both mutually orthogonal and all of unit 2-norm. \
({numref}`§ {number} <section-leastsq-qr>`)</br>

outer product
: Multiplication of two vectors resulting in a rank-1 matrix. \
({numref}`§ {number} <section-linsys-lu>`)</br>

overdetermined
: Characterized by having more constraints than available degrees of freedom. \
({numref}`§ {number} <section-leastsq-fitting>`)</br>

piecewise linear
: Function that is linear between each consecutive pair of nodes, but whose slope may jump at the nodes. \
({numref}`§ {number} <section-localapprox-pwlin>`)</br>

PLU factorization
: LU factorization with row pivoting. \
({numref}`§ {number} <section-linsys-pivoting>`)</br>

power iteration
: Repeated application of a matrix to a vector, followed by normalization, resulting in convergence to an eigenvector for the dominant eigenvalue. \
({numref}`§ {number} <section-krylov-power>`)</br>

preconditioning
: Use of an approximate inverse to improve the convergence rate of Krylov iterations for a linear system. \
({numref}`§ {number} <section-krylov-precond>`)</br>

pseudoinverse
: Rectangular matrix that maps data to solution in the linear least-squares problem, generalizing the matrix inverse. \
({numref}`§ {number} <section-leastsq-normaleqns>`)</br>

QR factorization
: Representation of a matrix as the product of an orthogonal and an upper triangular matrix. \
({numref}`§ {number} <section-leastsq-qr>`)</br>

quadratic convergence
: Sequence in which the difference between sequence value and limit asymptotically decreases by a constant times the square of the preceding difference. \
({numref}`§ {number} <section-nonlineqn-newton>`)</br>

quasi-Newton methods
: Rootfinding methods that overcome the issues of Jacobian computation and lack of global convergence in Newton's method. \
({numref}`§ {number} <section-nonlineqn-quasinewton>`)</br>

quasimatrix
: Collection of functions (such as orthogonal polynomials) that have algebraic parallels to columns of a matrix. \
({numref}`§ {number} <section-globalapprox-orthogonal>`)</br>

Rayleigh quotient
: Function of vectors that equals an eigenvalue when given its eigenvector as input. \
({numref}`§ {number} <section-matrixanaly-symm-eig>`)</br>

reduced QR factorization
: See *thin QR*. \
/br>

reduced SVD
: See *thin SVD*. \
/br>

residual
: For a linear system, the difference between $\mathbf{b}$ and $\mathbf{A}\tilde{\mathbf{x}}$ for a computed solution approximation $\tilde{\mathbf{x}}$. More generally, the actual value of a quantity that is made zero by an exact solution. \
({numref}`§ {number} <section-linsys-condition-number>`, {numref}`§ {number} <section-nonlineqn-rootproblem>`)</br>

restarting
: Technique used in GMRES to prevent the work per iteration and overall storage from growing uncontrollably. \
({numref}`§ {number} <section-krylov-gmres>`)</br>

rootfinding problem
: Finding the input value for a given function which makes that function zero. \
({numref}`§ {number} <section-nonlineqn-rootproblem>`)</br>

row pivoting
: Reordering rows during LU factorization to ensure that the factorization exists and can be computed stably. \
({numref}`§ {number} <section-linsys-pivoting>`)</br>

Runge phenomenon
: Manifestation of the instability of polynomial interpolation at equally spaced nodes as degree increases. \
({numref}`§ {number} <section-globalapprox-stability>`)</br>

Runge--Kutta
: One-step method for IVPs that evaluates the derivative of the solution more than once to advance a single step. \
({numref}`§ {number} <section-ivp-rk>`)</br>

secant method
: Scalar quasi-Newton method that uses a secant line rather than a tangent line to define a root estimate. \
({numref}`§ {number} <section-nonlineqn-secant>`)</br>

shooting
: Unstable technique for solving a boundary-value problem in which an initial value is sought for by a rootfinding algorithm. \
({numref}`§ {number} <section-bvp-shooting>`)</br>

simple root
: Root of a function at which the derivative of the function is nonzero. \
({numref}`§ {number} <section-nonlineqn-rootproblem>`)</br>

singular value decomposition (SVD)
: Expression of a matrix as a product of two orthogonal/unitary matrices and a nonnegative diagonal matrix. \
({numref}`§ {number} <section-matrixanaly-svd>`)</br>

sparse
: Describing a matrix that has mostly zero elements for structural reasons. \
({numref}`§ {number} <section-linsys-structure>`, {numref}`§ {number} <section-krylov-structure>`)</br>

spectral convergence
: Exponentially rapid decrease in error as the number of interpolation nodes increases, e.g., as observed in Chebyshev polynomial and trigonometric interpolation. \
({numref}`§ {number} <section-globalapprox-stability>`)</br>

stability region
: Region of the complex plane describing when numerical solution of a linear IVP is bounded as $t\to\infty$. \
({numref}`§ {number} <section-diffusion-absstab>`)</br>

step size
: Increment in time between successive solution values in a numerical IVP solver. \
({numref}`§ {number} <section-ivp-euler>`)</br>

stiff
: Describes an IVP in which stability is a greater restriction than accuracy for many solution methods, usually favoring the use of an implicit time stepping method. \
({numref}`§ {number} <section-ivp-implicit>`, {numref}`§ {number} <section-diffusion-stiffness>`)</br>

subtractive cancellation
: Growth in relative error that occurs when two numbers are added/subtracted to get a result that is much smaller in magnitude than the operands; also called *loss of significance* or *cancellation error*. \
({numref}`§ {number} <section-intro-conditioning>`)</br>

superlinear convergence
: Sequence for which the convergence is asymptotically faster than any linear rate. \
({numref}`§ {number} <section-nonlineqn-secant>`)</br>

symmetric matrix
: Square matrix that is equal to its transpose. \
({numref}`§ {number} <section-linsys-matrices>`)</br>

symmetric positive definite (SPD) matrix
: Matrix that is symmetric and positive definite, thereby permitting a Cholesky factorization. Correspondingly called hermitian positive definite in the complex case. \
({numref}`§ {number} <section-linsys-structure>`)</br>

tensor-product domain
: A domain that can be parameterized using variables that lay in a logical rectangle or cuboid; i.e., each variable independently varies in an interval. \
({numref}`§ {number} <section-twodim-tensorprod>`)</br>

thin QR factorization
: Variant of the QR factorization that discards information not needed to fully represent the original matrix. \
({numref}`§ {number} <section-leastsq-qr>`)</br>

thin SVD
: Variant of the singular value decomposition that discards information not needed to fully represent the original matrix. \
({numref}`§ {number} <section-matrixanaly-svd>`)</br>

trapezoid formula
: Numerical integration method resulting from integration of a piecewise linear interpolant. \
({numref}`§ {number} <section-localapprox-integration>`, {numref}`§ {number} <section-ivp-multistep>`)</br>

triangular matrix
: Matrix that is all zero either above (for lower triangular) or below (for upper triangular) the main diagonal. \
({numref}`§ {number} <section-linsys-linear-systems>`)</br>

tridiagonal matrix
: Matrix with nonzeros only on the main diagonal and the adjacent two diagonals. \
({numref}`§ {number} <section-linsys-structure>`)</br>

trigonometric interpolation
: Interpolation of a periodic function by a linear combination of real or complex trigonometric functions. \
({numref}`§ {number} <section-globalapprox-trig>`)</br>

truncation error
: Difference between an exact value and an approximation, such as one that truncates an infinite series. \
({numref}`§ {number} <section-localapprox-fd-converge>`, {numref}`§ {number} <section-localapprox-integration>`)</br>

unit triangular matrix
: Triangular matrix that has a 1 in every position on the main diagonal. \
({numref}`§ {number} <section-linsys-lu>`)</br>

unit vector
: A vector whose norm equals 1. \
({numref}`§ {number} <section-linsys-norms>`)</br>

unitary
: Square matrix with complex-valued entries whose columns are orthonormal. \
({numref}`§ {number} <section-matrixanaly-evd>`)</br>

unstable
: Allowing perturbations of the data to have much larger effects on the results than can be explained by the problem's condition number. \
({numref}`§ {number} <section-intro-stability>`)</br>

upper Hessenberg
: Describing a matrix that has nonzeros only in the upper triangle and first subdiagonal. \
({numref}`§ {number} <section-krylov-subspace>`)</br>

Vandermonde matrix
: Matrix whose columns are formed from elementwise powers of a given vector, important for polynomial interpolation and approximation of data. \
({numref}`§ {number} <section-linsys-polyinterp>`)</br>

weights
: Coefficients in a linear combination of function values in a finite-difference or integration method. \
({numref}`§ {number} <section-localapprox-finitediffs>`, {numref}`§ {number} <section-localapprox-integration>`, {numref}`§ {number} <section-globalapprox-integration>`)</br>

zero-stable
: Boundedness property of multistep methods that is required for convergence. \
({numref}`§ {number} <section-ivp-zerostability>`)</br>

:::
