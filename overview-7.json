{"version":2,"kind":"Article","sha256":"f97f8fb9be08f918eeb1a0027a4f2ffde9243eea701edf19fad538d8509bc990","slug":"overview-7","location":"/chapter8/overview.md","dependencies":[],"frontmatter":{"title":"8. Krylov methods in linear algebra","content_includes_title":false,"authors":[{"nameParsed":{"literal":"Tobin A. Driscoll","given":"Tobin A.","family":"Driscoll"},"name":"Tobin A. Driscoll","email":"driscoll@udel.edu","id":"contributors-myst-generated-uid-0","corresponding":true},{"nameParsed":{"literal":"Richard J. Braun","given":"Richard J.","family":"Braun"},"name":"Richard J. Braun","id":"contributors-myst-generated-uid-1"}],"github":"https://github.com/fncbook/fnc","numbering":{"heading_1":{"enabled":true},"heading_2":{"enabled":true},"heading_3":{"enabled":false}},"math":{"\\float":{"macro":"\\mathbb{F}"},"\\real":{"macro":"\\mathbb{R}"},"\\complex":{"macro":"\\mathbb{C}"},"\\nat":{"macro":"\\mathbb{N}"},"\\integer":{"macro":"\\mathbb{Z}"},"\\rmn":{"macro":"\\mathbb{R}^{#1 \\times #2}"},"\\dd":{"macro":"\\frac{d #1}{d #2}"},"\\ddd":{"macro":"\\frac{d^2 #1}{d #2^2}"},"\\pp":{"macro":"\\frac{\\partial #1}{\\partial #2}"},"\\ppp":{"macro":"\\frac{\\partial^2 #1}{\\partial #2^2}"},"\\ppdd":{"macro":"\\frac{\\partial^2 #1}{\\partial #2 \\partial #3}"},"\\abs":{"macro":"\\left\\lvert #1 \\right\\rvert"},"\\norm":{"macro":"\\left\\lVert #1 \\right\\rVert"},"\\twonorm":{"macro":"\\norm{#1}_2"},"\\onenorm":{"macro":"\\norm{#1}_1"},"\\infnorm":{"macro":"\\norm{#1}_\\infty"},"\\anynorm":{"macro":"\\norm{#1}_#2"},"\\innerprod":{"macro":"\\langle #1,#2 \\rangle"},"\\pr":{"macro":"^{(#1)}"},"\\kron":{"macro":"#1 \\otimes #2"},"\\eye":{"macro":"\\mathbf{e}_#1"},"\\meye":{"macro":"\\mathbf{I}"},"\\Qhat":{"macro":"\\hat{\\mathbf{Q}}"},"\\Rhat":{"macro":"\\hat{\\mathbf{R}}"},"\\bfalpha":{"macro":"\\mathbf{alpha}"},"\\bfdelta":{"macro":"\\mathbf{delta}"},"\\bfzero":{"macro":"\\boldsymbol{0}"},"\\macheps":{"macro":"\\epsilon_\\text{mach}"},"\\fl":{"macro":"\\operatorname{fl}"},"\\diag":{"macro":"\\operatorname{diag}"},"\\ign":{"macro":"\\operatorname{sign}"},"\\Re":{"macro":"\\operatorname{Re}"},"\\Im":{"macro":"\\operatorname{Im}"},"\\ee":{"macro":"\\times 10^"},"\\lnorm":{"macro":"\\|"},"\\rnorm":{"macro":"\\|"},"\\floor":{"macro":"\\lfloor#1\\rfloor"},"\\cI":{"macro":"\\mathcal{I}"},"\\mtx":{"macro":"\\operatorname{mtx}"},"\\myvec":{"macro":"\\operatorname{vec}"},"\\argmin":{"macro":"\\operatorname{argmin}"}},"abbreviations":{"IVP":"initial-value problem","BVP":"boundary-value problem","TPBVP":"two-point boundary-value problem","ONC":"matrix with orthonormal columns","SVD":"singular value decomposition","EVD":"eigenvalue decomposition","SPD":"symmetric positive definite","HPD":"hermitian positive definite","PDE":"partial differential equation","ODE":"ordinary differential equation"},"edit_url":"https://github.com/fncbook/fnc/blob/master/chapter8/overview.md","exports":[{"format":"md","filename":"overview.md","url":"/build/overview-a035acd4376da6a53ec6990b61cdaa37.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"container","kind":"quote","children":[{"type":"blockquote","children":[{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"I warn you not to underestimate my powers.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"pke6KgffIW"}],"key":"pVMZLsQOF4"}],"key":"Cwne6fnBCD"},{"type":"caption","children":[{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Luke Skywalker, ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"jCZFXBQ9xX"},{"type":"emphasis","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Return of the Jedi","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"o1x6BV8zhv"}],"key":"WQaV6cxlGL"}],"key":"U1yEH7FpsK"}],"key":"P8WlVL3pxp"}],"class":"epigraph","indexEntries":[{"entry":"Luke Skywalker","emphasis":false},{"entry":"Return of the Jedi","emphasis":false}],"label":"index-qr433YWN0F","identifier":"index-qr433ywn0f","html_id":"index-qr433ywn0f","key":"Bk9jn67vgs"},{"type":"paragraph","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"What are the implications of the ","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"w4VZSvU4fU"},{"type":"inlineMath","value":"O(n^3)","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(n^3)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\">n</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">3</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>","key":"MgGHuICnHK"},{"type":"text","value":" work requirements for solving linear systems? Suppose tomorrow your computer became a thousand times faster. (Historically this has taken about 15 years in the real world.) Assuming you are willing to wait just as long today as you were yesterday, the size of the linear system you can solve has gone up only by a factor of 10. Nice, but not nearly the jump that you got in hardware power. In fact, there is an odd paradox: faster computers make faster algorithms ","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"cYooCChLKD"},{"type":"emphasis","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"more","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"YCvgc908mp"}],"key":"hFNwnyDopK"},{"type":"text","value":" important, not less, because they demand that you work at larger values of ","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"Gus0THsvnH"},{"type":"inlineMath","value":"n","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span>","key":"oKktrRPZQc"},{"type":"text","value":", where asymptotic differences are large.","position":{"start":{"line":14,"column":1},"end":{"line":14,"column":1}},"key":"o0TpnOGfDP"}],"key":"VH4xjKYcvw"},{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"In practice the only reasonable way to deal with large matrices (at this writing, ","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"Odpc22WXZM"},{"type":"inlineMath","value":"n>10^4","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi><mo>&gt;</mo><mn>1</mn><msup><mn>0</mn><mn>4</mn></msup></mrow><annotation encoding=\"application/x-tex\">n&gt;10^4</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.5782em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">&gt;</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8141em;\"></span><span class=\"mord\">1</span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">4</span></span></span></span></span></span></span></span></span></span></span>","key":"TQxadx8P2F"},{"type":"text","value":" or so) is if they are sparse, or can be approximated sparsely. But LU factorization of a sparse matrix does not necessarily lead to sparse factors, particularly when row pivoting is required. The algorithm can be improved to be more sparse-aware, but we will not go into the details.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"BogQToAaqU"}],"key":"kja2zGhnSd"},{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Instead, we will replace LU factorization with an iterative algorithm. Unlike the LU factorization, iteration gives useful intermediate and continually improving results before the exact solution is found, allowing us to stop well before the nominal exact termination. More importantly, though, these iterations, based on an idea called ","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"RhxcWLdAe6"},{"type":"emphasis","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Krylov subspaces","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"vx5ieb95WS"}],"key":"T45E7dSomV"},{"type":"text","value":", allow us to fully exploit sparsity.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"vDRziBVZhH"}],"key":"AFLvy6g5NB"},{"type":"paragraph","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"Krylov subspace methods have two other advantages that are subtle but critically relevant to applications. One is that they allow us to do linear algebra ","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"uc5JQValyr"},{"type":"emphasis","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"even without having the relevant matrix","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"g5uL7I2Uap"}],"key":"jFT2I6cjnL"},{"type":"text","value":". This may sound undesirable or even impossible, but it exploits the connection between matrix-vector multiplication and a linear transformation. The other major advantage of Krylov subspace iterations is that they can exploit approximate inverses when they are available. These two features are among the most powerful ideas behind scientific computation today.","position":{"start":{"line":20,"column":1},"end":{"line":20,"column":1}},"key":"OOlTlkwMii"}],"key":"SC4HeCOxqW"}],"key":"WdJaIYa4NX"}],"key":"g59xoGQXyW"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Next steps","url":"/next-6","group":"Preface"},"next":{"title":"Sparsity and structure","url":"/structure-1","group":"Preface"}}},"domain":"http://localhost:3000"}