{"version":2,"kind":"Article","sha256":"2e36c254ac01ce9e50dd6f3e357503475ffadd7319419543d4bf677347a50b99","slug":"next-7","location":"/chapter8/next.md","dependencies":[],"frontmatter":{"title":"Next steps","content_includes_title":false,"authors":[{"nameParsed":{"literal":"Tobin A. Driscoll","given":"Tobin A.","family":"Driscoll"},"name":"Tobin A. Driscoll","email":"driscoll@udel.edu","id":"contributors-myst-generated-uid-0","corresponding":true},{"nameParsed":{"literal":"Richard J. Braun","given":"Richard J.","family":"Braun"},"name":"Richard J. Braun","id":"contributors-myst-generated-uid-1"}],"github":"https://github.com/fncbook/fnc","numbering":{"heading_1":{"enabled":true},"heading_2":{"enabled":true},"heading_3":{"enabled":false},"title":{"offset":1}},"math":{"\\float":{"macro":"\\mathbb{F}"},"\\real":{"macro":"\\mathbb{R}"},"\\complex":{"macro":"\\mathbb{C}"},"\\nat":{"macro":"\\mathbb{N}"},"\\integer":{"macro":"\\mathbb{Z}"},"\\rmn":{"macro":"\\mathbb{R}^{#1 \\times #2}"},"\\dd":{"macro":"\\frac{d #1}{d #2}"},"\\ddd":{"macro":"\\frac{d^2 #1}{d #2^2}"},"\\pp":{"macro":"\\frac{\\partial #1}{\\partial #2}"},"\\ppp":{"macro":"\\frac{\\partial^2 #1}{\\partial #2^2}"},"\\ppdd":{"macro":"\\frac{\\partial^2 #1}{\\partial #2 \\partial #3}"},"\\abs":{"macro":"\\left\\lvert #1 \\right\\rvert"},"\\norm":{"macro":"\\left\\lVert #1 \\right\\rVert"},"\\twonorm":{"macro":"\\norm{#1}_2"},"\\onenorm":{"macro":"\\norm{#1}_1"},"\\infnorm":{"macro":"\\norm{#1}_\\infty"},"\\anynorm":{"macro":"\\norm{#1}_#2"},"\\innerprod":{"macro":"\\langle #1,#2 \\rangle"},"\\pr":{"macro":"^{(#1)}"},"\\kron":{"macro":"#1 \\otimes #2"},"\\eye":{"macro":"\\mathbf{e}_#1"},"\\meye":{"macro":"\\mathbf{I}"},"\\Qhat":{"macro":"\\hat{\\mathbf{Q}}"},"\\Rhat":{"macro":"\\hat{\\mathbf{R}}"},"\\bfalpha":{"macro":"\\mathbf{alpha}"},"\\bfdelta":{"macro":"\\mathbf{delta}"},"\\bfzero":{"macro":"\\boldsymbol{0}"},"\\macheps":{"macro":"\\epsilon_\\text{mach}"},"\\fl":{"macro":"\\operatorname{fl}"},"\\diag":{"macro":"\\operatorname{diag}"},"\\ign":{"macro":"\\operatorname{sign}"},"\\Re":{"macro":"\\operatorname{Re}"},"\\Im":{"macro":"\\operatorname{Im}"},"\\ee":{"macro":"\\times 10^"},"\\lnorm":{"macro":"\\|"},"\\rnorm":{"macro":"\\|"},"\\floor":{"macro":"\\lfloor#1\\rfloor"},"\\cI":{"macro":"\\mathcal{I}"},"\\mtx":{"macro":"\\operatorname{mtx}"},"\\myvec":{"macro":"\\operatorname{vec}"},"\\argmin":{"macro":"\\operatorname{argmin}"}},"abbreviations":{"IVP":"initial-value problem","BVP":"boundary-value problem","TPBVP":"two-point boundary-value problem","ONC":"matrix with orthonormal columns","SVD":"singular value decomposition","EVD":"eigenvalue decomposition","SPD":"symmetric positive definite","HPD":"hermitian positive definite","PDE":"partial differential equation","ODE":"ordinary differential equation"},"edit_url":"https://github.com/fncbook/fnc/blob/master/chapter8/next.md","exports":[{"format":"md","filename":"next.md","url":"/build/next-4bfef688a40fc5c01d2b347c511c63a5.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"The iterative solution of large linear systems is a vast and difficult subject. A broad yet detailed introduction to the subject, including classical topics such as Jacobi and Gauss–Seidel methods not mentioned in this chapter, is ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"EpJHWySgZd"},{"type":"cite","kind":"narrative","label":"saadIterativeMethods2003","identifier":"saaditerativemethods2003","children":[{"type":"text","value":"Saad (2003)","key":"DkKYrdEWeV"}],"enumerator":"1","key":"eU0JKe3if7"},{"type":"text","value":". A more focused introduction to Krylov methods is given in ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ZOVhS4j1YP"},{"type":"cite","kind":"narrative","label":"vandervorstIterativeKrylov2003","identifier":"vandervorstiterativekrylov2003","children":[{"type":"text","value":"van der Vorst (2003)","key":"kNnmu6QmRf"}],"enumerator":"2","key":"KihNBNpKwt"},{"type":"text","value":".","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"hAb34nHH7Y"}],"key":"E7Q3P7cxOk"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The conjugate gradient method was originally intended to be a direct method.  Theoretically, the answer is found in ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"EI1lmnPJ81"},{"type":"inlineMath","value":"n","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span>","key":"DBE0bJb1Bu"},{"type":"text","value":" steps if there are ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"XaQsVCqUIw"},{"type":"inlineMath","value":"n","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span>","key":"fxEdqAlAbt"},{"type":"text","value":" unknowns if the arithmetic is perfect.  However, for floating-point arithmetic this result no longer holds.  The trouble is that as the method progresses, the succeeding search directions become closer to being dependent, and this causes problems for conditioning and floating-point computation.  The method was not successful until it came to be viewed as an iterative method that could be stopped once a reasonable approximation was reached.  The method was discovered by Hestenes and Stiefel independently, but they joined forces to publish the widely cited paper ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"nB0qjJiTWg"},{"type":"cite","kind":"narrative","label":"hestenesMethodsConjugate1952","identifier":"hestenesmethodsconjugate1952","children":[{"type":"text","value":"Hestenes & Stiefel (1952)","key":"TTSMQv5vhn"}],"enumerator":"3","key":"w4yd3YXo6j"},{"type":"text","value":" as part of an early research program in computing run by what was then called the (US) National Bureau of Standards (now called the National Institute of Standards and Technology).  It took until the 1970s for the method to catch on as a computational method (","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Xpbg4G395U"},{"type":"cite","kind":"narrative","label":"golubHistoryConjugate1989","identifier":"golubhistoryconjugate1989","children":[{"type":"text","value":"Golub & O'Leary (1989)","key":"Kbwl69Ag4D"}],"enumerator":"4","key":"hOSuvYI6Q7"},{"type":"text","value":").  The interested reader can visit the SIAM History Project’s articles at ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"qaXROy2smF"},{"type":"link","url":"http://history.siam.org/hestenes.htm","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"http://​history​.siam​.org​/hestenes​.htm","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"A3vDabkXY2"}],"urlSource":"http://history.siam.org/hestenes.htm","key":"zZXIiFZIi7"},{"type":"text","value":" to find an article by Hestenes that recounts the discovery (reprinted from ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"i10iPvrDB4"},{"type":"cite","kind":"narrative","label":"nashHistoryScientific1990","identifier":"nashhistoryscientific1990","children":[{"type":"text","value":"Nash (1990)","key":"aQWNeAY9Pn"}],"enumerator":"5","key":"j3hLcO0I9t"},{"type":"text","value":").","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"nZSDmWXpMz"}],"key":"OC1BP7gNVp"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"An important instance of matrix-free iteration occurs in using Newton’s method to solve a nonlinear system of equations. The key step is to solve a linear system with the Jacobian matrix, which can be expensive to compute. However, applying the Jacobian to a vector is equivalent to taking a derivative in the direction of that vector, which can be approximated by a single finite difference. This observation leads to the idea of ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"mD1wEavl2i"},{"type":"emphasis","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Newton–Krylov","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"wu4fkfb2Bf"}],"key":"FWwukuGg5R"},{"type":"text","value":" methods. A good introduction to this topic is given in ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"oKHpfBbcqd"},{"type":"cite","kind":"narrative","label":"knollJacobianfreeNewton2004","identifier":"knolljacobianfreenewton2004","children":[{"type":"text","value":"Knoll & Keyes (2004)","key":"VPYsDM2IWb"}],"enumerator":"6","key":"Gya9uodxc9"},{"type":"text","value":".","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"R0YxFuFXtE"}],"key":"ZQXjgIHPkp"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"For those not experienced with preconditioning, it can seem like something of an art.  The approach that works best very often depends on the application. Summaries of some approaches can be found in ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Iwppdnkdw1"},{"type":"cite","kind":"narrative","label":"quarteroniNumericalMathematics2007","identifier":"quarteroninumericalmathematics2007","children":[{"type":"text","value":"Quarteroni ","key":"gHEwEjYaLi"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"D8lLQRMbR9"}],"key":"sSQUC7koML"},{"type":"text","value":" (2007)","key":"mdi3njowtd"}],"enumerator":"7","key":"GJ7AiwCVkW"},{"type":"text","value":" and ","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"V1gSA2k7qI"},{"type":"cite","kind":"narrative","label":"trefethenNumericalLinear1997","identifier":"trefethennumericallinear1997","children":[{"type":"text","value":"Trefethen & Bau (1997)","key":"JAd3xv4OPg"}],"enumerator":"8","key":"gf035wZpTG"},{"type":"text","value":".","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"jPhRPmGyDl"}],"key":"lysuH66pUp"}],"key":"jT6kpqIct7"}],"key":"p7qN5Nq2DZ"},"references":{"cite":{"order":["saadIterativeMethods2003","vandervorstIterativeKrylov2003","hestenesMethodsConjugate1952","golubHistoryConjugate1989","nashHistoryScientific1990","knollJacobianfreeNewton2004","quarteroniNumericalMathematics2007","trefethenNumericalLinear1997"],"data":{"saadIterativeMethods2003":{"label":"saadIterativeMethods2003","enumerator":"1","html":"Saad, Y. (2003). <i>Iterative Methods for Sparse Linear Systems: Second Edition</i>. SIAM."},"vandervorstIterativeKrylov2003":{"label":"vandervorstIterativeKrylov2003","enumerator":"2","html":"van der Vorst, H. A. (2003). <i>Iterative Krylov Methods for Large Linear Systems</i>. Cambridge University Press."},"hestenesMethodsConjugate1952":{"label":"hestenesMethodsConjugate1952","enumerator":"3","doi":"10.6028/jres.049.044","html":"Hestenes, M. R., & Stiefel, E. (1952). Methods of Conjugate Gradients for Solving Linear Systems. <i>Journal of Research of the National Bureau of Standards</i>, <i>49</i>(6), 409. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.6028/jres.049.044\">10.6028/jres.049.044</a>","url":"https://doi.org/10.6028/jres.049.044"},"golubHistoryConjugate1989":{"label":"golubHistoryConjugate1989","enumerator":"4","doi":"10.1137/1031003","html":"Golub, G. H., & O’Leary, D. P. (1989). Some History of the Conjugate Gradient and Lanczos Algorithms: 1948–1976. <i>SIAM Review</i>, <i>31</i>(1), 50–102. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1137/1031003\">10.1137/1031003</a>","url":"https://doi.org/10.1137/1031003"},"nashHistoryScientific1990":{"label":"nashHistoryScientific1990","enumerator":"5","html":"Nash, S. (1990). <i>A History of Scientific Computing</i>. Addison-Wesley Publishing Company."},"knollJacobianfreeNewton2004":{"label":"knollJacobianfreeNewton2004","enumerator":"6","doi":"10.1016/j.jcp.2003.08.010","html":"Knoll, D. A., & Keyes, D. E. (2004). Jacobian-Free Newton–Krylov Methods: A Survey of Approaches and Applications. <i>Journal of Computational Physics</i>, <i>193</i>(2), 357–397. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://doi.org/10.1016/j.jcp.2003.08.010\">10.1016/j.jcp.2003.08.010</a>","url":"https://doi.org/10.1016/j.jcp.2003.08.010"},"quarteroniNumericalMathematics2007":{"label":"quarteroniNumericalMathematics2007","enumerator":"7","html":"Quarteroni, A., Sacco, R., & Saleri, F. (2007). <i>Numerical Mathematics</i>. Springer."},"trefethenNumericalLinear1997":{"label":"trefethenNumericalLinear1997","enumerator":"8","html":"Trefethen, L. N., & Bau, D. (1997). <i>Numerical Linear Algebra</i>. SIAM."}}}},"footer":{"navigation":{"prev":{"title":"Preconditioning","url":"/precond","group":"Preface"},"next":{"title":"9. Global function approximation","url":"/overview-8","group":"Preface"}}},"domain":"http://localhost:3000"}